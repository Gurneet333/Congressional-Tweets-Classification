{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name: Aditya Taori and Gurneet Chhabra\n",
    "### Course Name: Introduction to Statistical Machine Learning\n",
    "### Course Code: DSCC465\n",
    "### Assignment Name: Kaggle Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "import random\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.decomposition import PCA,TruncatedSVD\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up input directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"E:/UOR Notes/Stats for Machine Learning/Assignments/Kaggle Assignment/\"\n",
    "training_input_file = \"congressional_tweet_training_data.csv\"\n",
    "testing_input_file = \"congressional_tweet_test_data.csv\"\n",
    "training_input_path = input_dir+training_input_file\n",
    "testing_input_path = input_dir + testing_input_file\n",
    "random.seed(265)  #Setting random seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading CSV files Tweets Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>full_text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>year</th>\n",
       "      <th>party_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>b\"RT @KUSINews: One of our longtime viewers wa...</td>\n",
       "      <td>KUSI</td>\n",
       "      <td>10</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>258</td>\n",
       "      <td>b\"Today I'm urging the @CDCgov to immediately ...</td>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>111</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>b'Tomorrow, #MO03 seniors graduate from Calvar...</td>\n",
       "      <td>MO03</td>\n",
       "      <td>2</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>b'Congrats to #TeamUSA and Canton Native @JGre...</td>\n",
       "      <td>TeamUSA WorldJuniors</td>\n",
       "      <td>3</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>b'Pleased to support @amergateways at their Ju...</td>\n",
       "      <td>ImmigrantHeritageMonth</td>\n",
       "      <td>3</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   favorite_count                                          full_text  \\\n",
       "0               0  b\"RT @KUSINews: One of our longtime viewers wa...   \n",
       "1             258  b\"Today I'm urging the @CDCgov to immediately ...   \n",
       "2               0  b'Tomorrow, #MO03 seniors graduate from Calvar...   \n",
       "3               9  b'Congrats to #TeamUSA and Canton Native @JGre...   \n",
       "4               3  b'Pleased to support @amergateways at their Ju...   \n",
       "\n",
       "                 hashtags  retweet_count    year party_id  \n",
       "0                    KUSI             10  2017.0        R  \n",
       "1             Coronavirus            111  2020.0        R  \n",
       "2                    MO03              2  2014.0        R  \n",
       "3    TeamUSA WorldJuniors              3  2017.0        R  \n",
       "4  ImmigrantHeritageMonth              3  2019.0        D  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cong_tweets_data = pd.read_csv(training_input_path)\n",
    "cong_tweets_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "favorite_count        0\n",
       "full_text             0\n",
       "hashtags              0\n",
       "retweet_count         0\n",
       "year              18712\n",
       "party_id              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cong_tweets_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cong_tweets_data = cong_tweets_data.dropna(subset = [\"Cleaned_Text_WO_Stopwords\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>full_text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>year</th>\n",
       "      <th>party</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>b'#TaxReform improved the playing field for Am...</td>\n",
       "      <td>TaxReform</td>\n",
       "      <td>13</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>b'This #NativeWomensEqualPay Day, we recommit ...</td>\n",
       "      <td>NativeWomensEqualPay</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>b\"\\xe2\\x80\\x9cI became convinced that our gene...</td>\n",
       "      <td>MeToo ShatteringTheSilence</td>\n",
       "      <td>24</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>b'During #NationalAdoptionMonth, we honor the ...</td>\n",
       "      <td>NationalAdoptionMonth</td>\n",
       "      <td>2</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>b'Happy #AirborneDay to our @USArmy paratroope...</td>\n",
       "      <td>AirborneDay AirborneAllTheWay</td>\n",
       "      <td>7</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  favorite_count                                          full_text  \\\n",
       "0   0              70  b'#TaxReform improved the playing field for Am...   \n",
       "1   1              27  b'This #NativeWomensEqualPay Day, we recommit ...   \n",
       "2   2              49  b\"\\xe2\\x80\\x9cI became convinced that our gene...   \n",
       "3   3              14  b'During #NationalAdoptionMonth, we honor the ...   \n",
       "4   4              13  b'Happy #AirborneDay to our @USArmy paratroope...   \n",
       "\n",
       "                        hashtags  retweet_count    year party  \n",
       "0                      TaxReform             13  2018.0     D  \n",
       "1           NativeWomensEqualPay             11     NaN     D  \n",
       "2     MeToo ShatteringTheSilence             24  2017.0     D  \n",
       "3          NationalAdoptionMonth              2  2019.0     D  \n",
       "4  AirborneDay AirborneAllTheWay              7  2018.0     D  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cong_testing_data = pd.read_csv(testing_input_path)\n",
    "cong_testing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                   0\n",
       "favorite_count       0\n",
       "full_text            0\n",
       "hashtags             0\n",
       "retweet_count        0\n",
       "year              8347\n",
       "party                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cong_testing_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(tweets_text,sw_rem):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    #print(tweets_text)\n",
    "    #print(\"\\n\")\n",
    "    tweets_text = re.sub(\"b['\\\"]\",\"\",tweets_text)\n",
    "    tweets_text = re.sub('http\\S+',\"\",tweets_text)\n",
    "    #tweets_text = re.sub('#[A-Za-z0-9]*',\"\",tweets_text)\n",
    "    tweets_text = re.sub('@[A-Za-z0-9]*',\"\",tweets_text)\n",
    "    tweets_text = re.sub(r'[\\\\]n',\" \",tweets_text)\n",
    "    tweets_text = re.sub(r'[\\\\]x[a-f0-9][a-f0-9]',\"\",tweets_text)\n",
    "    tweets_text = re.sub(r'(&amp;)|RT ',\"\",tweets_text)\n",
    "    if sw_rem==1:\n",
    "        cachedStopWords = stopwords.words(\"english\")\n",
    "        regexes= r'\\b(' + r'|'.join(cachedStopWords) + r')\\b\\s*'\n",
    "        tweets_text = re.sub(regexes,\"\",tweets_text.lower())\n",
    "    new_string = tweets_text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokenized_string = new_string.split(\" \")\n",
    "    without_digit = [i for i in tokenized_string if not i.isdigit()]\n",
    "    no_small_words_text = [i for i in without_digit if len(i)>2]  #Removing short characters \n",
    "    lemmatized_string_list = [lemmatizer.lemmatize(word) for word in no_small_words_text if word!=\"\"]\n",
    "    lemmatized_string = \" \".join(lemmatized_string_list)\n",
    "    #print(new_string)\n",
    "    #print(\"\\n\")\n",
    "    return lemmatized_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>full_text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>year</th>\n",
       "      <th>party_id</th>\n",
       "      <th>Cleaned_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>b\"RT @KUSINews: One of our longtime viewers wa...</td>\n",
       "      <td>KUSI</td>\n",
       "      <td>10</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>R</td>\n",
       "      <td>One our longtime viewer wa Congressman office ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>258</td>\n",
       "      <td>b\"Today I'm urging the @CDCgov to immediately ...</td>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>111</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>R</td>\n",
       "      <td>Today urging the immediately launch phone hotl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>b'Tomorrow, #MO03 seniors graduate from Calvar...</td>\n",
       "      <td>MO03</td>\n",
       "      <td>2</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>R</td>\n",
       "      <td>Tomorrow MO03 senior graduate from Calvary Lut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>b'Congrats to #TeamUSA and Canton Native @JGre...</td>\n",
       "      <td>TeamUSA WorldJuniors</td>\n",
       "      <td>3</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>R</td>\n",
       "      <td>Congrats TeamUSA and Canton Native winning the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>b'Pleased to support @amergateways at their Ju...</td>\n",
       "      <td>ImmigrantHeritageMonth</td>\n",
       "      <td>3</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>D</td>\n",
       "      <td>Pleased support their June Fiesta which honore...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592798</th>\n",
       "      <td>3</td>\n",
       "      <td>b'This time, it focused on careers in #publics...</td>\n",
       "      <td>publicservice publicsafety</td>\n",
       "      <td>0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>R</td>\n",
       "      <td>This time focused career publicservice and pub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592799</th>\n",
       "      <td>5</td>\n",
       "      <td>b'.#StormyDaniels, #MichaelWolfe, #JamesComey ...</td>\n",
       "      <td>StormyDaniels MichaelWolfe JamesComey</td>\n",
       "      <td>1</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>R</td>\n",
       "      <td>StormyDaniels MichaelWolfe JamesComey Making t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592800</th>\n",
       "      <td>33</td>\n",
       "      <td>b'@NRDems The American people deserve the trut...</td>\n",
       "      <td>CultureOfCorruption</td>\n",
       "      <td>14</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>D</td>\n",
       "      <td>The American people deserve the truth and Cong...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592801</th>\n",
       "      <td>4</td>\n",
       "      <td>b'Only 2 weeks left to submit your #app to the...</td>\n",
       "      <td>app copolitics CAC16 HouseOfCode co06</td>\n",
       "      <td>3</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>R</td>\n",
       "      <td>Only week left submit your app the Congression...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592802</th>\n",
       "      <td>155</td>\n",
       "      <td>b'The #MuslimBan remains as un-American and of...</td>\n",
       "      <td>MuslimBan</td>\n",
       "      <td>48</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>D</td>\n",
       "      <td>The MuslimBan remains unAmerican and offensive...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>592803 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        favorite_count                                          full_text  \\\n",
       "0                    0  b\"RT @KUSINews: One of our longtime viewers wa...   \n",
       "1                  258  b\"Today I'm urging the @CDCgov to immediately ...   \n",
       "2                    0  b'Tomorrow, #MO03 seniors graduate from Calvar...   \n",
       "3                    9  b'Congrats to #TeamUSA and Canton Native @JGre...   \n",
       "4                    3  b'Pleased to support @amergateways at their Ju...   \n",
       "...                ...                                                ...   \n",
       "592798               3  b'This time, it focused on careers in #publics...   \n",
       "592799               5  b'.#StormyDaniels, #MichaelWolfe, #JamesComey ...   \n",
       "592800              33  b'@NRDems The American people deserve the trut...   \n",
       "592801               4  b'Only 2 weeks left to submit your #app to the...   \n",
       "592802             155  b'The #MuslimBan remains as un-American and of...   \n",
       "\n",
       "                                     hashtags  retweet_count    year party_id  \\\n",
       "0                                        KUSI             10  2017.0        R   \n",
       "1                                 Coronavirus            111  2020.0        R   \n",
       "2                                        MO03              2  2014.0        R   \n",
       "3                        TeamUSA WorldJuniors              3  2017.0        R   \n",
       "4                      ImmigrantHeritageMonth              3  2019.0        D   \n",
       "...                                       ...            ...     ...      ...   \n",
       "592798             publicservice publicsafety              0  2017.0        R   \n",
       "592799  StormyDaniels MichaelWolfe JamesComey              1  2018.0        R   \n",
       "592800                    CultureOfCorruption             14  2020.0        D   \n",
       "592801  app copolitics CAC16 HouseOfCode co06              3  2016.0        R   \n",
       "592802                              MuslimBan             48  2020.0        D   \n",
       "\n",
       "                                             Cleaned_Text  \n",
       "0       One our longtime viewer wa Congressman office ...  \n",
       "1       Today urging the immediately launch phone hotl...  \n",
       "2       Tomorrow MO03 senior graduate from Calvary Lut...  \n",
       "3       Congrats TeamUSA and Canton Native winning the...  \n",
       "4       Pleased support their June Fiesta which honore...  \n",
       "...                                                   ...  \n",
       "592798  This time focused career publicservice and pub...  \n",
       "592799  StormyDaniels MichaelWolfe JamesComey Making t...  \n",
       "592800  The American people deserve the truth and Cong...  \n",
       "592801  Only week left submit your app the Congression...  \n",
       "592802  The MuslimBan remains unAmerican and offensive...  \n",
       "\n",
       "[592803 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cong_tweets_data[\"Cleaned_Text\"] = cong_tweets_data.apply(lambda x:clean_tweets(x['full_text'],0),axis=1)\n",
    "cong_tweets_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cong_tweets_data[\"Cleaned_Text_WO_Stopwords\"].iloc[1871]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cong_tweets_data[\"Cleaned_Text_WO_Stopwords\"] = cong_tweets_data.apply(lambda x:clean_tweets(x['full_text'],1),axis=1)\n",
    "cong_tweets_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_clean_tweets = cong_tweets_data[\"Cleaned_Text\"].apply(len)\n",
    "cong_tweets_data[\"Cleaned_Tweets_Length\"] = len_clean_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_clean_tweets_wo_sw = cong_tweets_data[\"Cleaned_Text_WO_Stopwords\"].apply(len)\n",
    "cong_tweets_data[\"Cleaned_Tweets_WO_Stopwords_Length\"] = len_clean_tweets_wo_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cong_tweets_data.to_csv(\"Processed_Training_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = preprocessing.LabelEncoder()\n",
    " \n",
    "# Encode labels in column 'species'.\n",
    "cong_tweets_data['encoded_party']= label_encoder.fit_transform(cong_tweets_data[\"party_id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['D', 'R'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cong_tweets_data['encoded_party']\n",
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(cong_tweets_data[\"Cleaned_Text_WO_Stopwords\"],cong_tweets_data[\"encoded_party\"],test_size=0.2,shuffle=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(cong_tweets_data[\"Cleaned_Text\"],cong_tweets_data[\"encoded_party\"],test_size=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(474242, 165538)\n",
      "(118561, 165538)\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer1 = TfidfVectorizer(lowercase=True, ngram_range=(1, 1)) #object for ngram_range(1,1)\n",
    "X_train_vectors_tfidf = tfidf_vectorizer1.fit_transform(X_train) \n",
    "X_test_vectors_tfidf = tfidf_vectorizer1.transform(X_test)\n",
    "print(X_train_vectors_tfidf.shape)\n",
    "print(X_test_vectors_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.89      0.86     65025\n",
      "           1       0.85      0.80      0.82     53536\n",
      "\n",
      "    accuracy                           0.85    118561\n",
      "   macro avg       0.85      0.84      0.84    118561\n",
      "weighted avg       0.85      0.85      0.85    118561\n",
      "\n",
      "Confusion Matrix: [[57724  7301]\n",
      " [10964 42572]]\n"
     ]
    }
   ],
   "source": [
    "nb_clf = MultinomialNB()\n",
    "tfidf_vect_nb_model = nb_clf.fit(X_train_vectors_tfidf,y_train)\n",
    "tfidf_vect_nb_predictions = tfidf_vect_nb_model.predict(X_test_vectors_tfidf)\n",
    "tfidf_vect1_accuracy = accuracy_score(y_test,tfidf_vect_nb_predictions)\n",
    "#y_prob = tfidf_vect_nb_model.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "print(classification_report(y_test,tfidf_vect_nb_predictions))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, tfidf_vect_nb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.87      0.88     65025\n",
      "           1       0.85      0.87      0.86     53536\n",
      "\n",
      "    accuracy                           0.87    118561\n",
      "   macro avg       0.87      0.87      0.87    118561\n",
      "weighted avg       0.87      0.87      0.87    118561\n",
      "\n",
      "Confusion Matrix: [[56520  8505]\n",
      " [ 6708 46828]]\n"
     ]
    }
   ],
   "source": [
    "lr_clf = LogisticRegression(solver = 'liblinear', C=20, penalty = 'l2',class_weight='balanced')\n",
    "tfidf_vect_lr_model = lr_clf.fit(X_train_vectors_tfidf,y_train)\n",
    "tfidf_vect_lr_predictions = tfidf_vect_lr_model.predict(X_test_vectors_tfidf)\n",
    "tfidf_vect_lr_accuracy = accuracy_score(y_test,tfidf_vect_lr_predictions)\n",
    "y_prob = tfidf_vect_lr_model.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "print(classification_report(y_test,tfidf_vect_lr_predictions))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, tfidf_vect_lr_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = SVC()\n",
    "tfidf_vect_svm_model = svm_clf.fit(X_train_vectors_tfidf,y_train)\n",
    "tfidf_vect_svm_predictions = tfidf_vect_svm_model.predict(X_test_vectors_tfidf)\n",
    "tfidf_vect_svm_accuracy = accuracy_score(y_test,tfidf_vect_svm_predictions)\n",
    "y_prob = tfidf_vect_svm_model.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "print(classification_report(y_test,tfidf_vect_svm_predictions))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, tfidf_vect_svm_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators = 50, random_state = 42,max_depth=50,class_weight = \"balanced\")\n",
    "tfidf_vect_rf_model = rf_clf.fit(X_train_vectors_tfidf,y_train)\n",
    "tfidf_vect_rf_predictions = tfidf_vect_rf_model.predict(X_test_vectors_tfidf)\n",
    "tfidf_vect_rf_accuracy = accuracy_score(y_test,tfidf_vect_rf_predictions)\n",
    "y_prob = tfidf_vect_rf_model.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "print(classification_report(y_test,tfidf_vect_rf_predictions))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, tfidf_vect_rf_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(474242, 2363402)\n",
      "(118561, 2363402)\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer12 = TfidfVectorizer(lowercase=True, ngram_range=(1, 2)) #object for ngram_range(1,1)\n",
    "X_train_vectors_tfidf_12 = tfidf_vectorizer12.fit_transform(X_train) \n",
    "X_test_vectors_tfidf_12 = tfidf_vectorizer12.transform(X_test)\n",
    "print(X_train_vectors_tfidf_12.shape)\n",
    "print(X_test_vectors_tfidf_12.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.93      0.85     65025\n",
      "           1       0.89      0.68      0.77     53536\n",
      "\n",
      "    accuracy                           0.82    118561\n",
      "   macro avg       0.83      0.80      0.81    118561\n",
      "weighted avg       0.83      0.82      0.81    118561\n",
      "\n",
      "Confusion Matrix: [[60327  4698]\n",
      " [17099 36437]]\n"
     ]
    }
   ],
   "source": [
    "nb_clf = MultinomialNB()\n",
    "tfidf12_vect_nb_model = nb_clf.fit(X_train_vectors_tfidf_12,y_train)\n",
    "tfidf12_vect_nb_predictions = tfidf12_vect_nb_model.predict(X_test_vectors_tfidf_12)\n",
    "tfidf_vect1_accuracy = accuracy_score(y_test,tfidf12_vect_nb_predictions)\n",
    "#y_prob = tfidf_vect_nb_model.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "print(classification_report(y_test,tfidf12_vect_nb_predictions))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, tfidf12_vect_nb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.88     65025\n",
      "           1       0.86      0.85      0.86     53536\n",
      "\n",
      "    accuracy                           0.87    118561\n",
      "   macro avg       0.87      0.87      0.87    118561\n",
      "weighted avg       0.87      0.87      0.87    118561\n",
      "\n",
      "Confusion Matrix: [[57768  7257]\n",
      " [ 8105 45431]]\n"
     ]
    }
   ],
   "source": [
    "lr_clf_12 = LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')\n",
    "tfidf12_vect_lr_model = lr_clf_12.fit(X_train_vectors_tfidf_12,y_train)\n",
    "tfidf12_vect_lr_predictions = tfidf12_vect_lr_model.predict(X_test_vectors_tfidf_12)\n",
    "#tfidf_vect_lr_accuracy = accuracy_score(y_test,tfidf_vect_lr_predictions)\n",
    "#y_prob_12 = tfidf12_vect_lr_model.predict_proba(X_test_vectors_tfidf_12)[:,1]\n",
    "print(classification_report(y_test,tfidf12_vect_lr_predictions))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, tfidf12_vect_lr_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf_12 = RandomForestClassifier(n_estimators = 10, random_state = 42,class_weight = \"balanced\")\n",
    "tfidf12_vect_rf_model = rf_clf_12.fit(X_train_vectors_tfidf_12,y_train)\n",
    "tfidf12_vect_rf_predictions = tfidf12_vect_rf_model.predict(X_test_vectors_tfidf_12)\n",
    "#tfidf12_vect_rf_accuracy = accuracy_score(y_test,tfidf_vect_rf_predictions)\n",
    "#y_prob = tfidf_vect_rf_model.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "print(classification_report(y_test,tfidf12_vect_rf_predictions))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, tfidf12_vect_rf_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer13 = TfidfVectorizer(lowercase=True, ngram_range=(1, 3)) #object for ngram_range(1,1)\n",
    "X_train_vectors_tfidf_13 = tfidf_vectorizer13.fit_transform(X_train) \n",
    "X_test_vectors_tfidf_13 = tfidf_vectorizer13.transform(X_test)\n",
    "print(X_train_vectors_tfidf_13.shape)\n",
    "print(X_test_vectors_tfidf_13.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_clf_13 = MultinomialNB()\n",
    "tfidf13_vect_nb_model = nb_clf_13.fit(X_train_vectors_tfidf_13,y_train)\n",
    "tfidf13_vect_nb_predictions = tfidf13_vect_nb_model.predict(X_test_vectors_tfidf_13)\n",
    "#tfidf_vect1_accuracy = accuracy_score(y_test,tfidf13_vect_nb_predictions)\n",
    "#y_prob = tfidf_vect_nb_model.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "print(classification_report(y_test,tfidf13_vect_nb_predictions))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, tfidf13_vect_nb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf_13 = LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')\n",
    "tfidf13_vect_lr_model = lr_clf_13.fit(X_train_vectors_tfidf_13,y_train)\n",
    "tfidf13_vect_lr_predictions = tfidf13_vect_lr_model.predict(X_test_vectors_tfidf_13)\n",
    "#tfidf_vect_lr_accuracy = accuracy_score(y_test,tfidf_vect_lr_predictions)\n",
    "y_prob_13 = tfidf13_vect_lr_model.predict_proba(X_test_vectors_tfidf_13)[:,1]\n",
    "print(classification_report(y_test,tfidf13_vect_lr_predictions))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, tfidf13_vect_lr_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf_13 = RandomForestClassifier(n_estimators = 10, random_state = 42,class_weight = \"balanced\")\n",
    "tfidf13_vect_rf_model = rf_clf_13.fit(X_train_vectors_tfidf_13,y_train)\n",
    "tfidf13_vect_rf_predictions = tfidf13_vect_rf_model.predict(X_test_vectors_tfidf_13)\n",
    "#tfidf12_vect_rf_accuracy = accuracy_score(y_test,tfidf_vect_rf_predictions)\n",
    "#y_prob = tfidf_vect_rf_model.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "print(classification_report(y_test,tfidf13_vect_rf_predictions))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, tfidf13_vect_rf_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer22 = TfidfVectorizer(lowercase=True, ngram_range=(2, 2)) #object for ngram_range(1,1)\n",
    "X_train_vectors_tfidf_22 = tfidf_vectorizer22.fit_transform(X_train) \n",
    "X_test_vectors_tfidf_22 = tfidf_vectorizer22.transform(X_test)\n",
    "print(X_train_vectors_tfidf_22.shape)\n",
    "print(X_test_vectors_tfidf_22.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_clf_22 = MultinomialNB()\n",
    "tfidf22_vect_nb_model = nb_clf_22.fit(X_train_vectors_tfidf_22,y_train)\n",
    "tfidf22_vect_nb_predictions = tfidf22_vect_nb_model.predict(X_test_vectors_tfidf_22)\n",
    "#tfidf_vect1_accuracy = accuracy_score(y_test,tfidf13_vect_nb_predictions)\n",
    "#y_prob = tfidf_vect_nb_model.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "print(classification_report(y_test,tfidf22_vect_nb_predictions))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, tfidf22_vect_nb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf_22 = LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')\n",
    "tfidf22_vect_lr_model = lr_clf_22.fit(X_train_vectors_tfidf_22,y_train)\n",
    "tfidf22_vect_lr_predictions = tfidf22_vect_lr_model.predict(X_test_vectors_tfidf_22)\n",
    "#tfidf_vect_lr_accuracy = accuracy_score(y_test,tfidf_vect_lr_predictions)\n",
    "#y_prob_13 = tfidf13_vect_lr_model.predict_proba(X_test_vectors_tfidf_13)[:,1]\n",
    "print(classification_report(y_test,tfidf22_vect_lr_predictions))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, tfidf22_vect_lr_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cong_testing_data[\"Cleaned_Text_WO_Stopwords\"] = cong_testing_data.apply(lambda x:clean_tweets(x['full_text'],1),axis=1)\n",
    "cong_testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cong_testing_data.to_csv(\"Processed_Tesing_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = tfidf_vectorizer1.transform(cong_testing_data[\"Cleaned_Text_WO_Stopwords\"])\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pedictions = tfidf_vect_lr_model.predict(test_data)\n",
    "prediction_labels = label_encoder.inverse_transform(test_pedictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions_df = pd.DataFrame()\n",
    "submissions_df[\"Id\"] = cong_testing_data[\"Id\"]\n",
    "submissions_df[\"party\"] = prediction_labels\n",
    "submissions_df.to_csv(\"sample_submission_v1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = TruncatedSVD(1000)\n",
    "clf.fit(X_train_vectors_tfidf)\n",
    "pca_train_values = clf.transform(X_train_vectors_tfidf)\n",
    "pca_test_values = clf.transform(X_test_vectors_tfidf)\n",
    "pca_train_df = pd.DataFrame(pca_train_values)\n",
    "#pca_train_df.columns = [\"pca_dim_1\",\"pca_dim_2\"]\n",
    "pca_test_df = pd.DataFrame(pca_train_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test_vectors_tfidf.shape)\n",
    "print(X_train_vectors_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca_train_df.shape)\n",
    "print(pca_test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
